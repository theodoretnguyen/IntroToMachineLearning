\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin = 1in]{geometry}
\usepackage{amsfonts, amsmath, amssymb}
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\title{Yonsei Univeristy: Intro to Machine Learning}
\author{Professor Woong Lim \\ Notes by Theodore Nguyen}
\date{Summer 2022}


\begin{document}

\begin{titlepage}
    \maketitle
    \tableofcontents
\end{titlepage}

\section*{Week 1}
\addcontentsline{toc}{section}{Week 1}

\subsection*{1.1 Orientation}
\addcontentsline{toc}{subsection}{1.1 Orientation}
\subsubsection*{Goals}
\begin{itemize}
    \item Introduction to Machine Learning (ML).
    \item Learn the fundamentals of ML and apply select
    ML algorithms with big datasets.
    \item Review various techniques to build real-world AI applications.
\end{itemize}
\subsubsection*{Grading}
\begin{itemize}
    \item Daily and/or weekly mini-tasks: 70\%
    \item Project (1-2 big tasks) and Participation 30\%
\end{itemize}
\subsubsection*{Resources}
\begin{itemize}
    \item Intro to ML w/ Python by Muller \& Guido [Muller Text]
    \item The 100-page ML book by Burkov [Burkov Text]
    \item ML for absolute beginners [Beginner Text]
\end{itemize}

\subsection*{1.2 Overview and Getting Started}
\addcontentsline{toc}{subsection}{1.2 Overview and Getting Started}
\subsubsection*{Elements of Understanding and Doing ML}
\begin{itemize}
    \item The Problem: tasks, context, data, and potential solutions
    \item Python basics
    \item ML (or deep learning) algorithms
    \item Executing ML and evaluating efficacy
\end{itemize}
\subsubsection*{What is Machine Learning?}
\begin{enumerate}
    \item Building ML
    \begin{itemize}
        \item Algorithm
        \item Data (train, test)
    \end{itemize}
    \item ML Model 
    \begin{itemize}
        \item Input (parameters)
    \end{itemize}
    \item Output
    \begin{itemize}
        \item Decision: Prediction/Solution
    \end{itemize}
\end{enumerate}
\subsubsection*{Muller Textbook pg.\ 1-4}
\begin{itemize}
    \item Machine learning is about extracting knowledge from data. Also known as predictive analytics or statistical learning.
    \item The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalization from known examples.
    \begin{itemize}
        \item In this setting, which is known as \textit{supervised learning}, the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired output given an input. In particular, the algorithm is able to create an output for an input it has never seen before without any help from a human.
    \end{itemize}
    \item Machine learning algorithms that learn from input/output pairs are called supervised learning algorithms because a ``teacher'' provides supervision to the algorithms in the form of the desired outputs for each example that they learn from.
    \item Examples of supervised machine learning tasks:
    \begin{itemize}
        \item Identifying the zip code from handwritten digits on an envelope
        \begin{itemize}
            \item Input: scan of the handwriting. Output: actual digits in zip code.
        \end{itemize}
        \item Determining whether a tumor is benign based on a medical image
        \begin{itemize}
            \item Input: image. Output: whether the tumor is benign.
        \end{itemize}
        \item Detecting fraudulent activity in credit card transactions
        \begin{itemize}
            \item Input: a record of the credit card transaction. Output: whether it is likely to be fraudulent or not.
        \end{itemize}
    \end{itemize}
    \item In \textit{unsupervised learning}, only the input data is known, and no known output data is given to the algorithm.
    \item Examples of unsupervised learning:
    \begin{itemize}
        \item Identifying topics in a set of blog posts (might not know beforehand what these topics are, or how many topics there might be; hence there are no known outputs)
        \item Segmenting customers into groups with similar preferences (don't know in advance what these groups might be; hence no known outputs)
        \item Detecting abnormal access patterns to a website (only observe traffic to the website, so don't know what constitutes normal and abnormal behavior)
    \end{itemize}
    \item For both supervised and supervised learning tasks, it is important to have a representation of input data that a computer can understand
    \item Data Tables:
    \begin{itemize}
        \item Each data point that you want to reason about (each email, customer, transaction, etc.) is a \textbf{row}
        \begin{itemize}
            \item Each entity/row is known as a \underline{sample} (or data point)
        \end{itemize}
        \item Each property that describes the data point (age of a customer, amount of a transaction, etc.) is a \textbf{column}
        \begin{itemize}
            \item Each column/property is a \underline{feature}
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsubsection*{Learning: Hexagon}
\begin{enumerate}
    \item AI: can find hexagons
    \item ML: Write an algorithm and use train/test data to develop a model. There is ``labeling'' involved to help the algorithm to tell hexagons from other figures.
    \item Deep Learning: Generally, there is no labeling involved. Feedback (right or wrong) helps improve performance.
\end{enumerate}
\subsubsection*{Python-based ML environments}
\begin{enumerate}
    \item Package: Anaconda (about 3GB)
    \begin{enumerate}
        \item Jupyter Notebook (or Colab)
        \item Spyder (debugging)
        \item Otherlibraries
    \end{enumerate}
    \item Data/code cloud
    \begin{enumerate}
        \item Github
    \end{enumerate}
\end{enumerate}
\subsubsection*{Python Libraries}
\begin{enumerate}
    \item NumPy (numeric python): Vector, matrix
    \item Pandas: spreadsheet data
    \item Matplotlib, Seaborn: data visualization
    \item Scikit-learn: ML package (``playbook'')
\end{enumerate}

\subsection*{1.3 Key Terms}
\addcontentsline{toc}{subsection}{1.3 Key Terms}
\subsubsection*{Key Concepts}
\begin{itemize}
    \item Supervised, Unsupervised
    \item Classification, Regression
    \item Overfitting, Underfitting
    \item Confusion matrix
    \item Accuracy, precision, recall
    \item ML algorithms
\end{itemize}
\subsubsection*{Classification, Regression, Clustering, Underfitting, Overfitting}
\begin{itemize}
    \item \textbf{Classification}: Classify an item into a definitive category (supervised technique)
    \item \textbf{Regression}: Predicting a numerical value (mostly supervised)
    \item \textbf{Clustering}: Putting similar items together (unsupervised technique)
    \item \textbf{Underfitting}: few features
    \item \textbf{Overfitting}: too many features
\end{itemize}
\subsubsection*{Confusion Matrix: Evaluating ML Models}
\begin{center}
    \begin{tabular}{|l|l|l|l|l|}
        \hline
                    & \textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} \\ \hline
        \textbf{A} & 14         & 2          & 2          & 2          \\ \hline
        \textbf{B} & 2          & 18         & 0          & 0          \\ \hline
        \textbf{C} & 4          & 2          & 12         & 2          \\ \hline
        \textbf{D} & 1          & 1          & 2          & 16         \\ \hline
    \end{tabular}
\end{center}
\begin{itemize}
    \item In 20 trials, an ML model predicted:
    \begin{itemize}
        \item A correctly 14 times;
        \item B correctly 18 times;
        \item C correctly 12 times; and
        \item D correctly 16 times
    \end{itemize}
\end{itemize}
\[\text{Accuracy} = \frac{14+18+12+16}{20 \times 4} = 75\%\]
\subsubsection*{Accuracy, Precision, Recall}
\begin{center}
    \begin{tabular}{|l|l|l|}
        \hline
        \rowcolor[HTML]{3531FF} 
        {\color[HTML]{FFFFFF} ML Model}       & {\color[HTML]{FFFFFF} Disease} & {\color[HTML]{FFFFFF} No Disease} \\ \hline
        \cellcolor[HTML]{67FD9A}Test Positive & True Positive (TP)             & False Positive (FP)               \\ \hline
        \cellcolor[HTML]{FFCCC9}Test Negative & False Negative (FN)            & True Negative (TN)                \\ \hline
    \end{tabular}
\end{center}
\[\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FN} + \text{FP} + \text{TN}}\]
\[\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\]
\[\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\]
\subsubsection*{$f_1$-score (or $f$-measure):}
\[f_1\text{-score} = 2 \cdot \frac{\text{recall}\cdot\text{precision}}{\text{recall} + \text{precision}}\]
\[\text{(the harmonic mean of recall and position)}\]

\subsection*{1.4 K-nearest neighbor (KNN) Basics I}
\addcontentsline{toc}{subsection}{1.4 K-nearest neighbor (KNN) Basics I}
\subsubsection*{k-Nearest Neighbors (Muller Text pg.\ 35)}
\begin{itemize}
    \item To make a prediction for a new data point, the algorithm finds the closet data points in the training dataset--its ``nearest neighbors.''
    \item \textbf{k-Neighbors classification}
    \begin{itemize}
        \item In its simplest version, the $k$-NN algorithm only considers exactly one nearest neighbor, which is the cloest training data point to the point we want to make a prediction for. The prediction is then simply the known output for this training point.
        \item Instead of considering only the closest neighbor, we can also consider an arbitrary number, $k$, of neighbors. We use \emph{voting} to assign a label. For each test point, we count how many neighbors belong to class 0 and how many belong to class 1. We then assign the class that is more freuqent: in other words, the majority class among the $k$-nearest neighbors.
\begin{lstlisting}[language = Python]
# Split data into a training and a test set so we can evaluate generalization performance
from sklearn.model_selection import train_test_split
X, y = mglearn.datasets.make_forge()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)
\end{lstlisting}
\begin{lstlisting}[language = Python]
# Fit the classifier using the training set
clf.fit(X_train, y_train)
\end{lstlisting}
\begin{lstlisting}[language = Python]
# To make predictions on the test data, we call the predict method. For each data point in the test set, this computes its nearest neighbors in the training set and finds the most common class among these
print("Test set predictions: {}".format(clf.predict(X_test)))
\end{lstlisting}
\begin{lstlisting}[language = Python]
# To evaluate how well the model generalizes, call the score method with the test data together with the test labels
print("Test set accuracy: {:.2f}".format(clf.score(X_test,y_test)))
\end{lstlisting}
    \end{itemize}
    \item \textbf{Analyzing KNeighborsClassifier}
    \begin{itemize}
        \item For two-dimensional datasets, we can illustrate the prediction for all possible test points in the $xy$-plane. We color the plane according to the class that would be assigned to a point in this region. This lets us view the \emph{decision boundary}, which is the divide between where the algorithm assignes class 0 versus where it assigns class 1
\begin{lstlisting}[language = Python]
# the following code produces visualizations of the decision boundaries for one, three, and nine neighbors
fig, axes = plt.subplots(1, 3, figsize=(10, 3))

for n_neighbors, ax in zip([1, 3, 9], axes):
    # the fit method returns the object self, so we can instantiate
    # and fit in one line
    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y) 
    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4) 
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    ax.set_title("{} neighbor(s)".format(n_neighbors))
    ax.set_xlabel("feature 0")
    ax.set_ylabel("feature 1")
axes[0].legend(loc=3)
\end{lstlisting}
\begin{center}
    \includegraphics*[scale = 0.7]{fig2_6.png}
\end{center}
        \item A smoother boundary corresponds to a simpler model.
        \item Using few neighbors corresponds to high model complexity
        \item Using many neighbors corresponds to low model complexity
    \end{itemize}
\begin{lstlisting}[language = Python]
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, stratify=cancer.target, random_state=66)

training_accuracy = [] 
test_accuracy = []
# try n_neighbors from 1 to 10 
neighbors_settings = range(1, 11)
for n_neighbors in neighbors_settings:
    # build the model
    clf = KNeighborsClassifier(n_neighbors=n_neighbors) 
    clf.fit(X_train, y_train)
    # record training set accuracy 
    training_accuracy.append(clf.score(X_train, y_train)) 
    # record generalization accuracy 
    test_accuracy.append(clf.score(X_test, y_test))
plt.plot(neighbors_settings, training_accuracy, label="training accuracy")
plt.plot(neighbors_settings, test_accuracy, label="test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("n_neighbors")
plt.legend()
\end{lstlisting}
\begin{center}
    \includegraphics*[scale = 0.7]{fig2_7.png}
\end{center}
    \item \textbf{Strengths, weaknesses, and parameters}
    \begin{itemize}
        \item Two important parameters to \verb`KNeighbors` classifier: the number of neighbors and how you measure distance between data points
        \item When using the $k$-NN algorithm, it's important to preprocess your data
    \end{itemize}
\end{itemize}
\subsubsection*{Concept of KNN}
\begin{itemize}
    \item Where am I?
    \item Where do I belong?
    \item Asking around - feature checking in the space (i.e., real world, vector space)
\end{itemize}
\subsubsection*{Cross Validation Scores}
\begin{itemize}
    \item \verb|KNN_example_2.ipynb|
    \begin{itemize}
        \item The \verb|train.shape[0]| size is 80. We are taking half of the number of data, so \verb|max_k_range| is 40. Since our range is from 3 to \verb|max_k_range| (40) and we are skipping every 1, we have 37 k's.
        \item Since the \verb|cv| parameter is set to be $10$, we get $10$ cv scores for every k. Since we have $37$ $k$'s, we have $37 * 10 = 370$ cv scores. However, \verb|cross_validation_scores| is comprised of the average cv scores, so we have 37 values in \verb|cross_validation_scores|.
        \item \emph{When talking about cross validation score, we are referring to the \underline{mean} cross validation score.}
    \end{itemize}
\end{itemize}


\section*{Week 2}
\addcontentsline{toc}{section}{Week 2}

\subsection*{2.2 KNN 2}
\addcontentsline{toc}{subsection}{2.2 KNN 2}
\subsubsection*{K-Fold CV}
\begin{itemize}
    \item $k = 5$ (\textbf{cv = 5})
    \item Divide the training data into 5 big folds
    \item Divide each fold into 5 small folds
    \item Use 1 small fold for testing
    \item Repeat the procedure 5 times progressively
\end{itemize}

\subsection*{2.3 SVM 1}
\addcontentsline{toc}{subsection}{2.3 SVM 1}
\subsubsection*{The basic steps of SVM in 2D}
\begin{enumerate}
    \item Select \textbf{two} hyperplanes with no points between them
    \item Maximize their distance (the margin)
    \item Add the average line (half way between the two hyperplanes). This will be the decision boundary.
\end{enumerate}
\subsubsection*{Hard margins vs.\ Soft margins}
\begin{itemize}
    \item Soft margin: Penalty for non-separable cases
\end{itemize}
\subsubsection*{Nonlinear SVMs}
\begin{itemize}
    \item Linear Kernel
    \item Polynomial Kernel $d = 2$
    \item Polynomial Kernel $d = 5$
\end{itemize}
\subsubsection*{SVMs in 3D}
\begin{itemize}
    \item A hyperplane in $\mathbb{R}^2$ is a line 
    \item A hyperplane in $\mathbb{R}^3$ is a plane
\end{itemize}
\subsubsection*{Kernel Tricks}
\begin{itemize}
    \item Non-linearity
    \item Higher dimensions
    \item Mapping to different (higher) dimensions
    \item \textbf{Choosing the right kernel}:
    \begin{itemize}
        \item \emph{Polynomial Kernel; Gaussian Radial Basis Function (RBF); Laplace RBF Kernel; Sigmoid Kernel}
    \end{itemize}
\end{itemize}
\subsubsection*{Important Parameters}
\begin{itemize}
    \item \textbf{kernel: rbf} (the most common), n-degree \textbf{poly} or \textbf{sigmoid}
    \item \textbf{C: regularization parameter}
    \item \textbf{gamma}
\end{itemize}
\subsubsection*{Regularization Parameter (``C'' in python)}
\begin{itemize}
    \item the SVM optimization to avoid non-separable points
    \item Lower $c$ $\rightarrow$ wider margin
    \item Higher $c$ $\rightarrow$ narrower margin
\end{itemize}
\subsubsection*{Gamma}
\begin{itemize}
    \item The gamma parameter indicates how far the model considers points to make decision boundaries
    \begin{itemize}
        \item \underline{high} gamma
        \item \underline{low} gamma
    \end{itemize}
\end{itemize}
\begin{center}
    \begin{tabular}{|l|l|l|l|l|}
    \hline
                                              & \cellcolor[HTML]{C0C0C0}\textbf{Large Gamma} & \cellcolor[HTML]{C0C0C0}\textbf{Small Gamma} & \cellcolor[HTML]{C0C0C0}\textbf{Large C} & \cellcolor[HTML]{C0C0C0}\textbf{Small C} \\ \hline
    \cellcolor[HTML]{C0C0C0}\textbf{Variance} & Low                                          & High (overfitting)                           & High (overfitting)                       & Low                                      \\ \hline
    \cellcolor[HTML]{C0C0C0}\textbf{Bias}     & High (underfitting)                          & Low                                          & Low                                      & High (underfitting)                      \\ \hline
    \end{tabular}
\end{center}


\section*{Week 3}
\addcontentsline{toc}{section}{Week 3}

\subsection*{3.1 SVM Math 1}
\addcontentsline{toc}{subsection}{3.1 SVM Math 1}
\subsubsection*{Derivation}
\begin{enumerate}
    \item ``Naive'' decision boundary: $x + y = 1x + 1y = 0$
    \begin{itemize}
        \item Weight vector: $W = \begin{bmatrix} 1 & 1 \end{bmatrix}$
    \end{itemize}
    \item \[W^T \cdot X = 0\] \[W = \begin{bmatrix} 1 & 1 \end{bmatrix}\] \[X = \begin{bmatrix} x & y \end{bmatrix}\] \[\begin{bmatrix} 1 \\ 1 \end{bmatrix} \begin{bmatrix} x & y \end{bmatrix} \equiv 0\]
    \item Translating the line: $W^T \cdot X = b$
    \item Finding best hyperplanes:
    \begin{enumerate}
        \item $W^T \cdot X = b + 1$
        \item $W^T \cdot X = b - 1$
    \end{enumerate}
    \item Rewriting the form:
    \begin{itemize}
        \item Decision boundary: $W^T \cdot X - b = 0$
        \item Hyperplane class $+1$: $W^T \cdot X - b = +1$
        \item Hyperplnae class $-1$: $W^T \cdot X - b = -1$
    \end{itemize}
    \item Finding best margin (the width)
    \begin{itemize}
        \item Decision boundary: $a_1x + a_2y - b = 0$
        \item Hyperplane class $+1$: $a_1x + a_2y - b = +1$
        \item Hyperplane class $-1$: $a_1x + a_2y - b = -1$
        \item $W = \begin{bmatrix} a_1 & a_2 \end{bmatrix}$
        \begin{itemize}
            \item Recall: Distance from a point to line \[d = \frac{|ax_1 + by_1 + c|}{\sqrt{(a^2 + b^2)}}\] where the line is given by $ax + by + c = 0$, and the point of interest is $(x_1, y_1)$.
        \end{itemize}
        \item So, we have \[d = \frac{|a_1x_1 + a_2y_2 - b|}{\sqrt{a_1^2 + a_2^2}}.\]
        \item We know that \[a_1x_1 + a_2y_2 - b = -1,\] so 
        \begin{align*} d &= \frac{|-1|}{\sqrt{(a_1^2 + a_2^2)}} \\ &= \frac{1}{||W||.} \end{align*} This is the distance from one parallel hyperplane to the decision boundary. Thus, the distance from one parallel hyperplane to the other is $\frac{2}{||W||}$.
    \end{itemize}
\end{enumerate}
\begin{itemize}
    \item Trivial and key insights
    \begin{itemize}
        \item $W$ = weight
        \item $b$ = bias
        \item A ``parallel'' hyperplane \emph{must} contain at least one data point (i.e.\ support vector)
        \item In the algorithm of SVM, the two parallel hyperplanes should be equidistant from the decision boundary
        \begin{itemize}
            \item The margin is set up to handle (i.e.\ penalize) the error points mathematically.
            \item Optimally ``separating'' Hyperplane: the hyperplanes for which the margin of separation ``2d'' is maximized.
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection*{3.1 SVM Math 2}
\addcontentsline{toc}{subsection}{3.1 SVM Math 2}
\subsubsection*{More General Derivation}
\begin{enumerate}
    \item Finding best hyperplanes
    \begin{itemize}
        \item Decision boundary line: $ax + by = c$
        \item Parallel hyperplane above: $ax + by = c + f$
        \item Parallel hyperplane below: $ax + by = c - f$
    \end{itemize}
    \item Suppose our support vectors are $(2, 3)$ and $(5, 5)$.
    \item The distance from $(2, 3)$ to the decision boundary line $ax + by = c$ is \[D = \frac{|2a + 3b - c|}{\sqrt{a^2 + b^2}}.\]
    \begin{itemize}
        \item We know:
        \begin{itemize}
            \item The line $ax + by = c - f$ contains $(2, 3)$. Then $2a + 3b = c - f$.
            \item The line $ax + by = c + f$ contains $(5, 5)$. Then $5a + 5b = c + f$.
        \end{itemize}
    \end{itemize}
    \item Add our two equations: \[7a + 8b = 2c \Longleftrightarrow c = 3.5a + 4b.\]
    \item Plug into $D$: \begin{align*} D &= \frac{|2a + 3b - (3.5a + 4b)|}{\sqrt{a^2 + b^2}} \\ &= \frac{|-1.5a - b|}{\sqrt{a^2 + b^2}} \end{align*}
    \item Maximize $D$ using Wolfram Alpha Maximum Calculator. Let $D = f(a, b) = \frac{|-1.5a - b|}{\sqrt{a^2 + b^2}}$. From the calculator, we get $y = \frac{2x}{3} \implies b = \frac{2a}{3} \Longleftrightarrow a = \frac{3b}{2}$.
    \item Put it all together. We have \[a = \frac{3b}{2}\] \[c = 3.5a + 4b.\] Thus, plugging these two substitutions into our decision boundary line $ax + by = c$, we get \[\left(\frac{3}{2}b\right)x + by = 3.5 \left(\frac{3}{2}b\right) + 4b\] \[y = -1.5x + 9.25 \Longleftrightarrow 1.5x + y = 9.25\] where $a = 1.5$, $b = 1$, and $c = 9.25$.
\end{enumerate}
\begin{itemize}
    \item To find the equations of the parallel hyperplanes, namely $ax + by = c \pm f$, plug in $a$, $b$, $c$, and the support vector coordinates $(x, y)$, and solve for $f$.
    \item Additional notes:
    \begin{itemize}
        \item In the above example, we used the distance from $(2, 3)$ to the decision boundary. By symmetry of the other support vector $(5, 5)$ (\emph{since parallel hyperplanes should be equidistant from the decision boundary}), we could have also maximized the distance from $(5, 5)$ to the decision boundary.
        \item The distance from one parallel hyperplane to the other is equal to the distance from one support vector to the other.
        \item The decision boundary line is perpendicular to the line connecting the two support vectors. That is, the slope of the decision boundary line is the opposite reciprocal of the slope of the line connecting the two support vectors.
        \item The physical space between the two parallel hyperplanes is called the \emph{marginal space}
    \end{itemize}
\end{itemize}

\subsection*{3.2 Data Scaling}
\addcontentsline{toc}{subsection}{3.2 Data Scaling}
\begin{itemize}
    \item Why Standardization?
    \begin{itemize}
        \item Resets all features to a common scale keeping fundamental properties of data such as roder, comparative magnitude, etc.
    \end{itemize}
    \item Scikit-learn Data Scalers
    \begin{itemize}
        \item \textbf{StandardScaler}
        \item \textbf{MinMaxScaler}
        \item MaxAbsScaler
        \item \textbf{RobustScaler}
        \item Normalizer
    \end{itemize}
    \item \emph{For \underline{training} data: do fitting first and do transforming later}
    \item \emph{For \underline{testing} data:} \texttt{transform()}
\end{itemize}
\subsubsection*{\texttt{fit()} vs.\ \texttt{transform()}}
\begin{itemize}
    \item \texttt{fit()}: Calculating the mean and variance of each feature data
    \item \texttt{transform()}: Using the mean and variance from ``fitting'' and conduct scaling
\end{itemize}
\subsubsection*{StandardScaler}
\begin{itemize}
    \item Standardize the data
    \item = Reformat (scaling) data (feature) to have the \textbf{Standard Normal Distribution} with the mean $= 0$ and standard deviation $= 1$
\end{itemize}
\subsubsection*{MinMaxScaler}
\begin{itemize}
    \item Reformat (scaling) data (feature) to have the minimum value $= 0$ and maximum $= 1$
    \item Spread out the data over $[0, 1]$
\end{itemize}
\subsubsection*{RobustScaler}
\begin{itemize}
    \item Reformat (scaling) data (feature) to have the median $= 0$ and Interquartile range (IQR) $= 1$
    \item Formula: $\frac{\text{``feature''} - \text{median value}}{\text{IQR}}$ where IQR $= Q_3 - Q_1$
    \item Not sensitive to outliers
\end{itemize}

\end{document}